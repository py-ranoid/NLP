{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import collections\n",
    "import codecs\n",
    "import sklearn\n",
    "\n",
    "# training data\n",
    "train_data = [\n",
    "    ('Who is Shaka Khan?', {\n",
    "        'entities': [(7, 17, 'PERSON')]\n",
    "    }),\n",
    "    ('I like London and Berlin.', {\n",
    "        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "    })\n",
    "]\n",
    "    \n",
    "def train_ner(model=None, output_dir=None, n_iter=100, train_data=train_data):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            for text, annotations in train_data:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in train_data:\n",
    "        doc = nlp(text)\n",
    "        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(dataset_filepath):\n",
    "    token_count = collections.defaultdict(lambda: 0)\n",
    "    label_count = collections.defaultdict(lambda: 0)\n",
    "    character_count = collections.defaultdict(lambda: 0)\n",
    "\n",
    "    line_count = -1\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    new_token_sequence = []\n",
    "    new_label_sequence = []\n",
    "    if dataset_filepath:\n",
    "        f = codecs.open(dataset_filepath, 'r', 'UTF-8')\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "            line = line.strip().split(' ')\n",
    "            if len(line) == 0 or len(line[0]) == 0 or '-DOCSTART-' in line[0]:\n",
    "                if len(new_token_sequence) > 0:\n",
    "                    labels.append(new_label_sequence)\n",
    "                    tokens.append(new_token_sequence)\n",
    "                    new_token_sequence = []\n",
    "                    new_label_sequence = []\n",
    "                continue\n",
    "            token = str(line[0])\n",
    "            label = str(line[-1])\n",
    "            token_count[token] += 1\n",
    "            label_count[label] += 1\n",
    "\n",
    "            new_token_sequence.append(token)\n",
    "            new_label_sequence.append(label)\n",
    "\n",
    "            for character in token:\n",
    "                character_count[character] += 1\n",
    "\n",
    "            if line_count > 20: break# for debugging purposes\n",
    "\n",
    "        if len(new_token_sequence) > 0:\n",
    "            labels.append(new_label_sequence)\n",
    "            tokens.append(new_token_sequence)\n",
    "        f.close()\n",
    "    return labels, tokens, token_count, label_count, character_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER'], ['B-LOC', 'O'], ['O', 'B-ORG', 'I-ORG', 'O']]\n",
      "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22'], ['The', 'European', 'Commission', 'said']]\n"
     ]
    }
   ],
   "source": [
    "labels, tokens, _, _, _ = parse_dataset('en/train.txt')\n",
    "print(labels)\n",
    "print(tokens)\n",
    "# train_ner(n_iter=10, train_data=train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Using spaCy CLI for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[93m    Generated output file data/train.txt.json\u001b[0m\r\n",
      "    Created 1 documents\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy convert en/train.txt data -c ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[93m    Generated output file data/test.txt.json\u001b[0m\r\n",
      "    Created 1 documents\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy convert en/test.txt data -c ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[93m    Generated output file data/valid.txt.json\u001b[0m\r\n",
      "    Created 1 documents\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy convert en/valid.txt data -c ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout_from = 0.2 by default\n",
      "dropout_to = 0.2 by default\n",
      "dropout_decay = 0.0 by default\n",
      "batch_from = 1 by default\n",
      "batch_to = 16 by default\n",
      "batch_compound = 1.001 by default\n",
      "max_doc_len = 5000 by default\n",
      "beam_width = 1 by default\n",
      "beam_density = 0.0 by default\n",
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "learn_rate = 0.001 by default\n",
      "optimizer_B1 = 0.9 by default\n",
      "optimizer_B2 = 0.999 by default\n",
      "optimizer_eps = 1e-08 by default\n",
      "L2_penalty = 1e-06 by default\n",
      "grad_norm_clip = 1.0 by default\n",
      "parser_hidden_depth = 1 by default\n",
      "parser_maxout_pieces = 2 by default\n",
      "token_vector_width = 128 by default\n",
      "hidden_width = 200 by default\n",
      "embed_size = 7000 by default\n",
      "history_feats = 0 by default\n",
      "history_width = 0 by default\n",
      "Itn.\tP.Loss\tN.Loss\tUAS\tNER P.\tNER R.\tNER F.\tTag %\tToken %\n",
      "0\t0.000\t2455.070\t0.000\t81.650\t82.447\t82.047\t0.000\t100.000\t11058.4\t0.0           \n",
      "1\t0.000\t23.767\t0.000\t85.959\t86.957\t86.455\t0.000\t100.000\t10791.8\t0.0             \n",
      "2\t0.000\t14.371\t0.000\t87.707\t88.371\t88.038\t0.000\t100.000\t10678.3\t0.0             \n",
      "3\t0.000\t10.352\t0.000\t87.926\t88.118\t88.022\t0.000\t100.000\t10083.6\t0.0             \n",
      "4\t0.000\t8.804\t0.000\t88.713\t88.489\t88.601\t0.000\t100.000\t10327.3\t0.0              \n",
      " 31%|██████████▌                       | 63272/204567 [00:50<01:23, 1685.80it/s]^C\n",
      "Saving model...                                                                 \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/SHURUI/.pyenv/versions/3.5.1/lib/python3.5/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/SHURUI/.pyenv/versions/3.5.1/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/SHURUI/.pyenv/versions/3.5.1/lib/python3.5/site-packages/spacy/__main__.py\", line 31, in <module>\n",
      "    plac.call(commands[command], sys.argv[1:])\n",
      "  File \"/Users/SHURUI/.pyenv/versions/3.5.1/lib/python3.5/site-packages/plac_core.py\", line 328, in call\n",
      "    cmd, result = parser.consume(arglist)\n",
      "  File \"/Users/SHURUI/.pyenv/versions/3.5.1/lib/python3.5/site-packages/plac_core.py\", line 207, in consume\n",
      "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
      "  File \"/Users/SHURUI/.pyenv/versions/3.5.1/lib/python3.5/site-packages/spacy/cli/train.py\", line 133, in train\n",
      "    drop=next(dropout_rates), losses=losses)\n",
      "  File \"/Users/SHURUI/.pyenv/versions/3.5.1/lib/python3.5/site-packages/spacy/language.py\", line 427, in update\n",
      "    proc.update(docs, golds, drop=drop, sgd=get_grads, losses=losses)\n",
      "  File \"nn_parser.pyx\", line 565, in spacy.syntax.nn_parser.Parser.update\n",
      "  File \"nn_parser.pyx\", line 730, in spacy.syntax.nn_parser.Parser.get_batch_model\n",
      "  File \"nn_parser.pyx\", line 85, in spacy.syntax.nn_parser.precompute_hiddens.__init__\n",
      "  File \"/Users/SHURUI/.pyenv/versions/3.5.1/lib/python3.5/site-packages/spacy/_ml.py\", line 149, in begin_update\n",
      "    self.W.reshape((self.nF*self.nO*self.nP, self.nI)).T)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train en model data/train.txt.json data/valid.txt.json -G -T -P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \u001b[93mResults\u001b[0m\n",
      "\n",
      "    Time               2.89 s         \n",
      "    Words              46666          \n",
      "    NER P              76.84          \n",
      "    TOK                100.00         \n",
      "    LAS                0.00           \n",
      "    NER R              78.17          \n",
      "    NER F              77.50          \n",
      "    UAS                0.00           \n",
      "    Words/s            16130          \n",
      "    POS                0.00           \n",
      "\n",
      "\n",
      "\u001b[93m    Generated 25 parses as HTML\u001b[0m\n",
      "    result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate model/model4 data/test.txt.json -dp result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
