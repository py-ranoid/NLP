{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def atisfold(fold):\n",
    "    assert fold in range(5)\n",
    "    f = PREFIX + 'atis.fold'+str(fold)+'.pkl'\n",
    "    train_set, valid_set, test_set, dicts = pickle.load(open(f, 'rb'), encoding='bytes')\n",
    "    return train_set, valid_set, test_set, dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'dataset/'\n",
    "w2ne, w2la = {}, {}\n",
    "train, _, test, dic = atisfold(1)\n",
    "w2idx, ne2idx, labels2idx = dic[b'words2idx'], dic[b'tables2idx'], dic[b'labels2idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([b'tables2idx', b'words2idx', b'labels2idx'])\n"
     ]
    }
   ],
   "source": [
    "print(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'aircraft', 'is', 'used', 'on', 'delta', 'flight', 'DIGITDIGITDIGITDIGIT', 'from', 'kansas', 'city', 'to', 'salt', 'lake', 'city']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-airline_name', 'O', 'B-flight_number', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'I-toloc.city_name']\n",
      "\n",
      "['i', 'want', 'to', 'go', 'from', 'boston', 'to', 'atlanta', 'on', 'monday']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'O', 'B-depart_date.day_name']\n",
      "\n",
      "['i', 'need', 'a', 'flight', 'from', 'atlanta', 'to', 'philadelphia', 'and', 'i', \"'m\", 'looking', 'for', 'the', 'cheapest', 'fare']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'O', 'O', 'O', 'O', 'O', 'O', 'B-cost_relative', 'O']\n",
      "\n",
      "['i', 'need', 'a', 'flight', 'from', 'toronto', 'to', 'montreal', 'reaching', 'montreal', 'early', 'on', 'friday']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'O', 'B-toloc.city_name', 'B-arrive_time.period_mod', 'O', 'B-arrive_date.day_name']\n",
      "\n",
      "['show', 'me', 'the', 'evening', 'flights', 'from', 'philadelphia', 'to', 'baltimore']\n",
      "['O', 'O', 'O', 'B-depart_time.period_of_day', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name']\n",
      "\n",
      "['tell', 'me', 'distance', 'from', 'orlando', 'airport', 'to', 'the', 'city']\n",
      "['O', 'O', 'O', 'O', 'B-fromloc.airport_name', 'I-fromloc.airport_name', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_entities(labels):\n",
    "    idx = 0\n",
    "    last_begin = -1\n",
    "    entity = \"\"\n",
    "    entities = []\n",
    "    while idx < len(labels):\n",
    "        if labels[idx].startswith('B'):\n",
    "            last_begin = idx;\n",
    "            entity = labels[idx][2:]\n",
    "        elif labels[idx].startswith('O'):\n",
    "            if last_begin > 0:\n",
    "                entities.append((last_begin, idx, entity))\n",
    "                last_begin = -1\n",
    "        idx += 1\n",
    "    if last_begin > 0:\n",
    "        entities.append((last_begin, idx, entity))\n",
    "\n",
    "    return entities\n",
    "\n",
    "from mitie import *\n",
    "idx2w  = dict((v,k) for k,v in w2idx.items())\n",
    "idx2ne = dict((v,k) for k,v in ne2idx.items())\n",
    "idx2la = dict((v,k) for k,v in labels2idx.items())\n",
    "\n",
    "test_x,  test_ne,  test_label  = test\n",
    "train_x, train_ne, train_label = train\n",
    "trainer = ner_trainer(\"../MITIE-models/english/total_word_feature_extractor.dat\")\n",
    "\n",
    "output = 0\n",
    "for sentence_a, label_a in zip(train_x, train_label):\n",
    "    instance = [idx2w[word].decode('utf8') for word in sentence_a]\n",
    "    labels = [idx2la[label].decode('utf8') for label in label_a]\n",
    "    sample = ner_training_instance(instance)\n",
    "    print(instance)\n",
    "    print(labels)\n",
    "    print()\n",
    "    for entity in get_entities(labels):\n",
    "        sample.add_entity(xrange(entity[0], entity[1]), entity[2])\n",
    "    trainer.add(sample)\n",
    "    output += 1\n",
    "    if output > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.num_threads = 4\n",
    "\n",
    "ner_atis = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags: ['airline_name', 'flight_number', 'fromloc.city_name', 'toloc.city_name', 'depart_date.day_name', 'cost_relative', 'arrive_time.period_mod', 'arrive_date.day_name', 'depart_time.period_of_day', 'fromloc.airport_name']\n"
     ]
    }
   ],
   "source": [
    "print (\"tags:\", ner_atis.get_possible_ner_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading NER model...\n",
      "\n",
      "Tags output by this NER model: ['PERSON', 'LOCATION', 'ORGANIZATION', 'MISC']\n"
     ]
    }
   ],
   "source": [
    "print(\"loading NER model...\")\n",
    "ner = named_entity_extractor(\"../MITIE-models/english/ner_model.dat\")\n",
    "print(\"\\nTags output by this NER model:\", ner.get_possible_ner_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner:\n",
      "\n",
      "Number of entities detected: 2\n",
      "\n",
      "    LOCATION: charlotte\n",
      "    LOCATION: las vegas\n",
      "\n",
      "ner_atis:\n",
      "\n",
      "Number of entities detected: 5\n",
      "\n",
      "    fromloc.city_name: like\n",
      "    depart_time.period_of_day: a\n",
      "    fromloc.city_name: charlotte\n",
      "    toloc.city_name: las vegas\n",
      "    toloc.city_name: louis\n",
      "ner:\n",
      "\n",
      "Number of entities detected: 2\n",
      "\n",
      "    LOCATION: tacoma\n",
      "    LOCATION: san\n",
      "\n",
      "ner_atis:\n",
      "\n",
      "Number of entities detected: 6\n",
      "\n",
      "    airline_name: april\n",
      "    fromloc.city_name: first\n",
      "    fromloc.city_name: tacoma\n",
      "    toloc.city_name: san\n",
      "    toloc.city_name: jose\n",
      "    toloc.city_name: departing\n",
      "ner:\n",
      "\n",
      "Number of entities detected: 2\n",
      "\n",
      "    LOCATION: phoenix\n",
      "    LOCATION: san diego\n",
      "\n",
      "ner_atis:\n",
      "\n",
      "Number of entities detected: 4\n",
      "\n",
      "    airline_name: april\n",
      "    fromloc.city_name: first\n",
      "    fromloc.city_name: phoenix\n",
      "    toloc.city_name: san diego\n"
     ]
    }
   ],
   "source": [
    "output = 0\n",
    "for sentence_a, label_a in zip(test_x, test_label):\n",
    "    tokens = [idx2w[word].decode('utf8') for word in sentence_a]\n",
    "    labels = [idx2la[label].decode('utf8') for label in label_a]\n",
    "    entities = ner.extract_entities(tokens)\n",
    "    print (\"ner:\")\n",
    "#     print (\"\\nSentence: \", tokens)\n",
    "#     print (\"\\nTest Label:\", labels)\n",
    "#     print (\"\\nEntities found:\", entities)\n",
    "#     print (\"\\nTest results:\", get_entities(labels))\n",
    "    print (\"\\nNumber of entities detected:\", len(entities))\n",
    "    print ()\n",
    "    \n",
    "    for e in entities:\n",
    "        range = e[0]\n",
    "        tag = e[1]\n",
    "        entity_text = \" \".join(tokens[i] for i in range)\n",
    "        print (\"    \" + tag + \": \" + entity_text)\n",
    "    \n",
    "    entities = ner_atis.extract_entities(tokens)\n",
    "    print (\"\\nner_atis:\")\n",
    "#     print (\"\\nSentence: \", tokens)\n",
    "#     print (\"\\nTest Label:\", labels)\n",
    "#     print (\"\\nEntities found:\", entities)\n",
    "#     print (\"\\nTest results:\", get_entities(labels))\n",
    "    print (\"\\nNumber of entities detected:\", len(entities))\n",
    "    print ()\n",
    "    for e in entities:\n",
    "        range = e[0]\n",
    "        tag = e[1]\n",
    "        entity_text = \" \".join(tokens[i] for i in range)\n",
    "        print (\"    \" + tag + \": \" + entity_text)\n",
    "    output += 1\n",
    "    if output > 2: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MITIE pretrained model tags: ['PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'], basically it could extract the information based on these tags.\n",
    "ATIS data gets a more descriptive tags, we could take more time to get a model to increase accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
