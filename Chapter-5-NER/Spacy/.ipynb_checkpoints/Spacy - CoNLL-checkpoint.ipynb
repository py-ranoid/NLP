{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import collections\n",
    "import codecs\n",
    "import sklearn\n",
    "\n",
    "# training data\n",
    "train_data = [\n",
    "    ('Who is Shaka Khan?', {\n",
    "        'entities': [(7, 17, 'PERSON')]\n",
    "    }),\n",
    "    ('I like London and Berlin.', {\n",
    "        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "    })\n",
    "]\n",
    "    \n",
    "def train_ner(model=None, output_dir=None, n_iter=100, train_data=train_data):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            for text, annotations in train_data:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in train_data:\n",
    "        doc = nlp(text)\n",
    "        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(dataset_filepath):\n",
    "    token_count = collections.defaultdict(lambda: 0)\n",
    "    label_count = collections.defaultdict(lambda: 0)\n",
    "    character_count = collections.defaultdict(lambda: 0)\n",
    "\n",
    "    line_count = -1\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    new_token_sequence = []\n",
    "    new_label_sequence = []\n",
    "    if dataset_filepath:\n",
    "        f = codecs.open(dataset_filepath, 'r', 'UTF-8')\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "            line = line.strip().split(' ')\n",
    "            if len(line) == 0 or len(line[0]) == 0 or '-DOCSTART-' in line[0]:\n",
    "                if len(new_token_sequence) > 0:\n",
    "                    labels.append(new_label_sequence)\n",
    "                    tokens.append(new_token_sequence)\n",
    "                    new_token_sequence = []\n",
    "                    new_label_sequence = []\n",
    "                continue\n",
    "            token = str(line[0])\n",
    "            label = str(line[-1])\n",
    "            token_count[token] += 1\n",
    "            label_count[label] += 1\n",
    "\n",
    "            new_token_sequence.append(token)\n",
    "            new_label_sequence.append(label)\n",
    "\n",
    "            for character in token:\n",
    "                character_count[character] += 1\n",
    "\n",
    "            if line_count > 20: break# for debugging purposes\n",
    "\n",
    "        if len(new_token_sequence) > 0:\n",
    "            labels.append(new_label_sequence)\n",
    "            tokens.append(new_token_sequence)\n",
    "        f.close()\n",
    "    return labels, tokens, token_count, label_count, character_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER'], ['B-LOC', 'O'], ['O', 'B-ORG', 'I-ORG', 'O']]\n",
      "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22'], ['The', 'European', 'Commission', 'said']]\n"
     ]
    }
   ],
   "source": [
    "labels, tokens, _, _, _ = parse_dataset('en/train.txt')\n",
    "print(labels)\n",
    "print(tokens)\n",
    "# train_ner(n_iter=10, train_data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_indices(labels, tokens, dataset_types):\n",
    "    tokens = self.tokens\n",
    "    labels = self.labels\n",
    "    token_to_index = self.token_to_index\n",
    "    character_to_index = self.character_to_index\n",
    "    label_to_index = self.label_to_index\n",
    "    index_to_label = self.index_to_label\n",
    "\n",
    "    # Map tokens and labels to their indices\n",
    "    token_indices = {}\n",
    "    label_indices = {}\n",
    "    characters = {}\n",
    "    token_lengths = {}\n",
    "    character_indices = {}\n",
    "    character_indices_padded = {}\n",
    "    for dataset_type in dataset_types:\n",
    "        token_indices[dataset_type] = []\n",
    "        characters[dataset_type] = []\n",
    "        character_indices[dataset_type] = []\n",
    "        token_lengths[dataset_type] = []\n",
    "        character_indices_padded[dataset_type] = []\n",
    "        for token_sequence in tokens[dataset_type]:\n",
    "            token_indices[dataset_type].append([token_to_index.get(token, self.UNK_TOKEN_INDEX) for token in token_sequence])\n",
    "            characters[dataset_type].append([list(token) for token in token_sequence])\n",
    "            character_indices[dataset_type].append([[character_to_index.get(character, random.randint(1, max(self.index_to_character.keys()))) for character in token] for token in token_sequence])\n",
    "            token_lengths[dataset_type].append([len(token) for token in token_sequence])\n",
    "            longest_token_length_in_sequence = max(token_lengths[dataset_type][-1])\n",
    "            character_indices_padded[dataset_type].append([utils.pad_list(temp_token_indices, longest_token_length_in_sequence, self.PADDING_CHARACTER_INDEX) for temp_token_indices in character_indices[dataset_type][-1]])\n",
    "\n",
    "        label_indices[dataset_type] = []\n",
    "        for label_sequence in labels[dataset_type]:\n",
    "            label_indices[dataset_type].append([label_to_index[label] for label in label_sequence])\n",
    "\n",
    "    # [Numpy 1-hot array](http://stackoverflow.com/a/42263603/395857)\n",
    "    label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
    "    label_binarizer.fit(range(max(index_to_label.keys()) + 1))\n",
    "    label_vector_indices = {}\n",
    "    for dataset_type in dataset_types:\n",
    "        label_vector_indices[dataset_type] = []\n",
    "        for label_indices_sequence in label_indices[dataset_type]:\n",
    "            label_vector_indices[dataset_type].append(label_binarizer.transform(label_indices_sequence))\n",
    "\n",
    "\n",
    "    return token_indices, label_indices, character_indices_padded, character_indices, token_lengths, characters, label_vector_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[93m    Unknown format\u001b[0m\r\n",
      "    Can't find converter for txt\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy convert en/train.txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
