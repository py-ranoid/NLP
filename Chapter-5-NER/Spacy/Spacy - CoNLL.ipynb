{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import collections\n",
    "import codecs\n",
    "import sklearn\n",
    "\n",
    "# training data\n",
    "train_data = [\n",
    "    ('Who is Shaka Khan?', {\n",
    "        'entities': [(7, 17, 'PERSON')]\n",
    "    }),\n",
    "    ('I like London and Berlin.', {\n",
    "        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "    })\n",
    "]\n",
    "    \n",
    "def train_ner(model=None, output_dir=None, n_iter=100, train_data=train_data):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            for text, annotations in train_data:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in train_data:\n",
    "        doc = nlp(text)\n",
    "        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(dataset_filepath):\n",
    "    token_count = collections.defaultdict(lambda: 0)\n",
    "    label_count = collections.defaultdict(lambda: 0)\n",
    "    character_count = collections.defaultdict(lambda: 0)\n",
    "\n",
    "    line_count = -1\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    new_token_sequence = []\n",
    "    new_label_sequence = []\n",
    "    if dataset_filepath:\n",
    "        f = codecs.open(dataset_filepath, 'r', 'UTF-8')\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "            line = line.strip().split(' ')\n",
    "            if len(line) == 0 or len(line[0]) == 0 or '-DOCSTART-' in line[0]:\n",
    "                if len(new_token_sequence) > 0:\n",
    "                    labels.append(new_label_sequence)\n",
    "                    tokens.append(new_token_sequence)\n",
    "                    new_token_sequence = []\n",
    "                    new_label_sequence = []\n",
    "                continue\n",
    "            token = str(line[0])\n",
    "            label = str(line[-1])\n",
    "            token_count[token] += 1\n",
    "            label_count[label] += 1\n",
    "\n",
    "            new_token_sequence.append(token)\n",
    "            new_label_sequence.append(label)\n",
    "\n",
    "            for character in token:\n",
    "                character_count[character] += 1\n",
    "\n",
    "            if line_count > 20: break# for debugging purposes\n",
    "\n",
    "        if len(new_token_sequence) > 0:\n",
    "            labels.append(new_label_sequence)\n",
    "            tokens.append(new_token_sequence)\n",
    "        f.close()\n",
    "    return labels, tokens, token_count, label_count, character_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER'], ['B-LOC', 'O'], ['O', 'B-ORG', 'I-ORG', 'O']]\n",
      "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22'], ['The', 'European', 'Commission', 'said']]\n"
     ]
    }
   ],
   "source": [
    "labels, tokens, _, _, _ = parse_dataset('en/train.txt')\n",
    "print(labels)\n",
    "print(tokens)\n",
    "# train_ner(n_iter=10, train_data=train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Using spaCy CLI for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[93m    Generated output file data/train.txt.json\u001b[0m\r\n",
      "    Created 1 documents\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy convert en/train.txt data -c ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[93m    Generated output file data/test.txt.json\u001b[0m\r\n",
      "    Created 1 documents\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy convert en/test.txt data -c ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[93m    Generated output file data/valid.txt.json\u001b[0m\r\n",
      "    Created 1 documents\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy convert en/valid.txt data -c ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout_from = 0.2 by default\n",
      "dropout_to = 0.2 by default\n",
      "dropout_decay = 0.0 by default\n",
      "batch_from = 1 by default\n",
      "batch_to = 16 by default\n",
      "batch_compound = 1.001 by default\n",
      "max_doc_len = 5000 by default\n",
      "beam_width = 1 by default\n",
      "beam_density = 0.0 by default\n",
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "learn_rate = 0.001 by default\n",
      "optimizer_B1 = 0.9 by default\n",
      "optimizer_B2 = 0.999 by default\n",
      "optimizer_eps = 1e-08 by default\n",
      "L2_penalty = 1e-06 by default\n",
      "grad_norm_clip = 1.0 by default\n",
      "parser_hidden_depth = 1 by default\n",
      "parser_maxout_pieces = 2 by default\n",
      "token_vector_width = 128 by default\n",
      "hidden_width = 200 by default\n",
      "embed_size = 7000 by default\n",
      "history_feats = 0 by default\n",
      "history_width = 0 by default\n",
      "Itn.\tP.Loss\tN.Loss\tUAS\tNER P.\tNER R.\tNER F.\tTag %\tToken %\n",
      "0\t0.000\t2458.585\t0.000\t82.434\t83.002\t82.717\t0.000\t100.000\t10261.4\t0.0           \n",
      "1\t0.000\t23.592\t0.000\t86.970\t87.058\t87.014\t0.000\t100.000\t10117.8\t0.0             \n",
      "2\t0.000\t14.454\t0.000\t87.329\t88.034\t87.680\t0.000\t100.000\t10034.5\t0.0             \n",
      "3\t0.000\t10.749\t0.000\t87.975\t88.405\t88.189\t0.000\t100.000\t9894.0\t0.0              \n",
      "4\t0.000\t9.022\t0.000\t88.521\t88.640\t88.581\t0.000\t100.000\t10335.9\t0.0              \n",
      "5\t0.000\t7.672\t0.000\t87.162\t87.866\t87.513\t0.000\t100.000\t10918.8\t0.0              \n",
      "6\t0.000\t6.585\t0.000\t87.726\t88.287\t88.005\t0.000\t100.000\t10691.1\t0.0              \n",
      "7\t0.000\t5.672\t0.000\t87.801\t88.304\t88.052\t0.000\t100.000\t10758.6\t0.0              \n",
      "8\t0.000\t5.334\t0.000\t88.437\t88.169\t88.303\t0.000\t100.000\t10398.2\t0.0              \n",
      "9\t0.000\t4.768\t0.000\t87.973\t88.388\t88.180\t0.000\t100.000\t10912.2\t0.0              \n",
      "10\t0.000\t4.853\t0.000\t88.853\t88.270\t88.561\t0.000\t100.000\t11703.7\t0.0             \n",
      "11\t0.000\t4.544\t0.000\t88.709\t88.590\t88.649\t0.000\t100.000\t10534.7\t0.0             \n",
      "12\t0.000\t4.046\t0.000\t88.544\t88.455\t88.500\t0.000\t100.000\t10997.0\t0.0             \n",
      "13\t0.000\t3.362\t0.000\t88.179\t87.748\t87.963\t0.000\t100.000\t10578.1\t0.0             \n",
      "14\t0.000\t3.514\t0.000\t88.269\t87.883\t88.076\t0.000\t100.000\t10838.6\t0.0             \n",
      "15\t0.000\t3.345\t0.000\t87.955\t87.748\t87.852\t0.000\t100.000\t10540.1\t0.0             \n",
      "16\t0.000\t3.290\t0.000\t87.886\t88.034\t87.960\t0.000\t100.000\t10622.4\t0.0             \n",
      "17\t0.000\t3.075\t0.000\t88.109\t88.287\t88.198\t0.000\t100.000\t10771.7\t0.0             \n",
      "18\t0.000\t2.911\t0.000\t88.167\t88.152\t88.160\t0.000\t100.000\t10596.7\t0.0             \n",
      "19\t0.000\t2.750\t0.000\t88.320\t88.186\t88.253\t0.000\t100.000\t10298.9\t0.0             \n",
      "20\t0.000\t2.941\t0.000\t87.886\t88.034\t87.960\t0.000\t100.000\t10023.6\t0.0             \n",
      "21\t0.000\t2.586\t0.000\t88.283\t88.253\t88.268\t0.000\t100.000\t10961.3\t0.0             \n",
      "22\t0.000\t2.570\t0.000\t88.095\t88.169\t88.132\t0.000\t100.000\t11000.8\t0.0             \n",
      "23\t0.000\t2.494\t0.000\t87.946\t88.287\t88.116\t0.000\t100.000\t10952.2\t0.0             \n",
      "24\t0.000\t2.598\t0.000\t88.275\t88.186\t88.230\t0.000\t100.000\t10613.8\t0.0             \n",
      "25\t0.000\t2.364\t0.000\t88.315\t88.018\t88.166\t0.000\t100.000\t10119.3\t0.0             \n",
      "26\t0.000\t2.271\t0.000\t88.060\t88.001\t88.030\t0.000\t100.000\t10922.7\t0.0             \n",
      "27\t0.000\t2.070\t0.000\t87.563\t87.799\t87.681\t0.000\t100.000\t10141.4\t0.0             \n",
      "28\t0.000\t2.153\t0.000\t87.948\t88.051\t87.999\t0.000\t100.000\t10948.0\t0.0             \n",
      "29\t0.000\t2.213\t0.000\t88.186\t87.933\t88.059\t0.000\t100.000\t10694.4\t0.0             \n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train en model data/train.txt.json data/valid.txt.json -G -T -P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \u001b[93mResults\u001b[0m\n",
      "\n",
      "    NER R              77.71          \n",
      "    NER P              76.82          \n",
      "    LAS                0.00           \n",
      "    TOK                100.00         \n",
      "    Time               3.06 s         \n",
      "    Words/s            15259          \n",
      "    UAS                0.00           \n",
      "    NER F              77.26          \n",
      "    Words              46666          \n",
      "    POS                0.00           \n",
      "\n",
      "\n",
      "\u001b[93m    Generated 25 parses as HTML\u001b[0m\n",
      "    result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate model/model-final data/test.txt.json -dp result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
