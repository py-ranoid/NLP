{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stanfordcorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('../stanford-corenlp-full-2018-02-27') \n",
    "# this server needs root to run notebook server while running\n",
    "# ex. sudo jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize: ['Guangdong', 'University', 'of', 'Foreign', 'Studies', 'is', 'located', 'in', 'Guangzhou', '.']\n",
      "Part of Speech: [('Guangdong', 'NNP'), ('University', 'NNP'), ('of', 'IN'), ('Foreign', 'NNP'), ('Studies', 'NNPS'), ('is', 'VBZ'), ('located', 'JJ'), ('in', 'IN'), ('Guangzhou', 'NNP'), ('.', '.')]\n",
      "Named Entities: [('Guangdong', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('of', 'ORGANIZATION'), ('Foreign', 'ORGANIZATION'), ('Studies', 'ORGANIZATION'), ('is', 'O'), ('located', 'O'), ('in', 'O'), ('Guangzhou', 'CITY'), ('.', 'O')]\n",
      "Constituency Parsing: (ROOT\n",
      "  (S\n",
      "    (NP\n",
      "      (NP (NNP Guangdong) (NNP University))\n",
      "      (PP (IN of)\n",
      "        (NP (NNP Foreign) (NNPS Studies))))\n",
      "    (VP (VBZ is)\n",
      "      (ADJP (JJ located)\n",
      "        (PP (IN in)\n",
      "          (NP (NNP Guangzhou)))))\n",
      "    (. .)))\n",
      "Dependency Parsing: [('ROOT', 0, 7), ('compound', 2, 1), ('nsubjpass', 7, 2), ('case', 5, 3), ('compound', 5, 4), ('nmod', 2, 5), ('auxpass', 7, 6), ('case', 9, 8), ('nmod', 7, 9), ('punct', 7, 10)]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Guangdong University of Foreign Studies is located in Guangzhou.'\n",
    "print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "print('Named Entities:', nlp.ner(sentence))\n",
    "print('Constituency Parsing:', nlp.parse(sentence))\n",
    "print('Dependency Parsing:', nlp.dependency_parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset from CoNLL 2003 (using spacy code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _consume_os(tags):\n",
    "    ## reference: https://github.com/explosion/spaCy/blob/c7d53348d7c0474852dc5ebe5794f2816ef7eb01/spacy/gold.pyx\n",
    "    while tags and tags[0] == 'O':\n",
    "        yield tags.pop(0)\n",
    "\n",
    "\n",
    "def _consume_ent(tags):\n",
    "    if not tags:\n",
    "        return []\n",
    "    tag = tags.pop(0)\n",
    "    target_in = 'I' + tag[1:]\n",
    "    target_last = 'L' + tag[1:]\n",
    "    length = 1\n",
    "    while tags and tags[0] in {target_in, target_last}:\n",
    "        length += 1\n",
    "        tags.pop(0)\n",
    "    label = tag[2:]\n",
    "    if length == 1:\n",
    "        return ['U-' + label]\n",
    "    else:\n",
    "        start = 'B-' + label\n",
    "        end = 'L-' + label\n",
    "        middle = ['I-%s' % label for _ in range(1, length - 1)]\n",
    "        return [start] + middle + [end]\n",
    "    \n",
    "def iob_to_biluo(tags):\n",
    "    out = []\n",
    "    curr_label = None\n",
    "    tags = list(tags)\n",
    "    while tags:\n",
    "        out.extend(_consume_os(tags))\n",
    "        out.extend(_consume_ent(tags))\n",
    "    return out\n",
    "\n",
    "def read_conll_ner(input_path):\n",
    "    ## reference: https://github.com/explosion/spaCy/blob/master/spacy/cli/converters/conll_ner2json.py\n",
    "    text = open(input_path,'r', encoding='utf-8').read()\n",
    "    i = 0\n",
    "    delimit_docs = '-DOCSTART- -X- O O'\n",
    "    output_docs = []\n",
    "    for doc in text.strip().split(delimit_docs):\n",
    "        doc = doc.strip()\n",
    "        if not doc:\n",
    "            continue\n",
    "        output_doc = []\n",
    "        for sent in doc.split('\\n\\n'):\n",
    "            sent = sent.strip()\n",
    "            if not sent:\n",
    "                continue\n",
    "            lines = [line.strip() for line in sent.split('\\n') if line.strip()]\n",
    "            words, tags, chunks, iob_ents = zip(*[line.split() for line in lines])\n",
    "            biluo_ents = iob_to_biluo(iob_ents)\n",
    "            output_doc.append({'tokens': [\n",
    "                {'orth': w, 'tag': tag, 'ner': ent} for (w, tag, ent) in\n",
    "                zip(words, tags, biluo_ents)\n",
    "            ]})\n",
    "        output_docs.append({\n",
    "            'id': len(output_docs),\n",
    "            'paragraphs': [{'sentences': output_doc}]\n",
    "        })\n",
    "        output_doc = []\n",
    "    return output_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_conll_ner('CoNLL - 2003/en/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [{'orth': 'SOCCER', 'tag': 'NN', 'ner': 'O'}, {'orth': '-', 'tag': ':', 'ner': 'O'}, {'orth': 'JAPAN', 'tag': 'NNP', 'ner': 'U-LOC'}, {'orth': 'GET', 'tag': 'VB', 'ner': 'O'}, {'orth': 'LUCKY', 'tag': 'NNP', 'ner': 'O'}, {'orth': 'WIN', 'tag': 'NNP', 'ner': 'O'}, {'orth': ',', 'tag': ',', 'ner': 'O'}, {'orth': 'CHINA', 'tag': 'NNP', 'ner': 'U-PER'}, {'orth': 'IN', 'tag': 'IN', 'ner': 'O'}, {'orth': 'SURPRISE', 'tag': 'DT', 'ner': 'O'}, {'orth': 'DEFEAT', 'tag': 'NN', 'ner': 'O'}, {'orth': '.', 'tag': '.', 'ner': 'O'}]}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0]['paragraphs'][0]['sentences'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract fisrt sentence from test CoNLL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token['orth'] for token in test_data[0]['paragraphs'][0]['sentences'][1]['tokens']]\n",
    "sentence = ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of Speech: [('SOCCER', 'NN'), ('-', ':'), ('JAPAN', 'NNP'), ('GET', 'VBP'), ('LUCKY', 'JJ'), ('WIN', 'NN'), (',', ','), ('CHINA', 'NNP'), ('IN', 'IN'), ('SURPRISE', 'NNP'), ('DEFEAT', 'NNP'), ('.', '.')]\n",
      "Named Entities: [('SOCCER', 'O'), ('-', 'O'), ('JAPAN', 'COUNTRY'), ('GET', 'O'), ('LUCKY', 'O'), ('WIN', 'O'), (',', 'O'), ('CHINA', 'COUNTRY'), ('IN', 'O'), ('SURPRISE', 'O'), ('DEFEAT', 'O'), ('.', 'O')]\n",
      "Constituency Parsing: (ROOT\n",
      "  (FRAG\n",
      "    (NP (NN SOCCER))\n",
      "    (: -)\n",
      "    (S\n",
      "      (NP (NNP JAPAN))\n",
      "      (VP (VBP GET)\n",
      "        (NP\n",
      "          (NP (JJ LUCKY) (NN WIN))\n",
      "          (, ,)\n",
      "          (NP\n",
      "            (NP (NNP CHINA))\n",
      "            (PP (IN IN)\n",
      "              (NP (NNP SURPRISE) (NNP DEFEAT)))))))\n",
      "    (. .)))\n",
      "Dependency Parsing: [('ROOT', 0, 1), ('punct', 1, 2), ('nsubj', 4, 3), ('dep', 1, 4), ('amod', 6, 5), ('dobj', 4, 6), ('punct', 4, 7), ('dep', 4, 8), ('case', 11, 9), ('compound', 11, 10), ('nmod', 8, 11), ('punct', 1, 12)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "print('Named Entities:', nlp.ner(sentence))\n",
    "print('Constituency Parsing:', nlp.parse(sentence))\n",
    "print('Dependency Parsing:', nlp.dependency_parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StanfordCoreNLP' object has no attribute 'relation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ae2987daccd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Relation Extractor:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'StanfordCoreNLP' object has no attribute 'relation'"
     ]
    }
   ],
   "source": [
    "print('Relation Extractor:', nlp.relation(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
